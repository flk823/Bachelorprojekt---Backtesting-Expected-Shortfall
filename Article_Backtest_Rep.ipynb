{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.special as sc\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "from scipy.stats import gaussian_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VaR og ES baseret p√• t-fordelingen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR(alpha = 0.975, sigma2 = 1, mu = 0, shift = 0, scale=1, df=None, type='Normal' , Norm = False):\n",
    "    if Norm == False:\n",
    "        if type == 'Normal':\n",
    "            VaR = mu+np.sqrt(sigma2)*norm.ppf(1-alpha)\n",
    "\n",
    "        elif type == 't':\n",
    "            VaR = t.ppf(1-alpha, df, loc=shift, scale = scale)\n",
    "    elif Norm == True:\n",
    "        if type == 'Normal':\n",
    "            VaR = norm.ppf(1-alpha)\n",
    "\n",
    "        elif type == 't':\n",
    "            VaR = np.sqrt((df-2)/df)*t.ppf(1-alpha, df, loc=shift, scale = scale)\n",
    "    return -VaR\n",
    "\n",
    "\n",
    "\n",
    "def ES(alpha = 0.975, sigma2 = 1, mu = 0, shift = 0, scale = 1, df=None, type='Normal' , Norm = False):\n",
    "    if Norm == False:\n",
    "        if type == 'Normal':\n",
    "            ES = (mu-np.sqrt(sigma2)/(1-alpha)*norm.pdf(norm.ppf(1-alpha)))*scale+shift\n",
    "        \n",
    "        elif type == 't':\n",
    "            x = t.ppf(1-alpha, df)\n",
    "            ES = -(t.pdf(x, df)/(1-alpha)*(df+x**2)/(df-1))*scale+shift\n",
    "    \n",
    "    elif Norm == True:\n",
    "        if type == 'Normal':\n",
    "            ES = (mu-np.sqrt(sigma2)/(1-alpha)*norm.pdf(norm.ppf(1-alpha)))*scale+shift\n",
    "        \n",
    "        elif type == 't':\n",
    "            x = t.ppf(1-alpha, df)\n",
    "            ES = -np.sqrt((df-2)/df)*(t.pdf(x, df)/(1-alpha)*(df+x**2)/(df-1))*scale+shift\n",
    "    \n",
    "    return -ES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3378027922014133"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ES(type = 'Normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed VaR_2.5 for df = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.141191751112397"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shift(alpha, df=None, dfnull=100): #insert significance level, sigma^2, mu and degrees of freedom\n",
    "    VaR1 = VaR(1-alpha,df = dfnull, Norm = False, type = 't')\n",
    "    VaR2 = VaR(1-alpha,df = df, Norm = False, type = 't')\n",
    "    res = VaR1-VaR2\n",
    "    return res\n",
    "\n",
    "ES(alpha = 0.975 , shift = shift(0.975, df=5, dfnull = 100) , df = 5, type = 't', Norm = True) #Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.703467135483324"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scale(signi = 0.05, df = 100, type = 'Normal'):\n",
    "    res = ES(alpha = 0.975 , sigma2 = 1,mu = 0, df = df, type = type)/ES(alpha = 1-signi , sigma2 = 1,mu = 0, df = df, type = type)\n",
    "    return res\n",
    "\n",
    "ES(alpha = 0.975 , shift = 0 , scale  = scale(0.05 , 100, 't'), df = 100, type = 't') #Sanity check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VaR 99% Backtest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VaR_backtest(sigma2, mu, T, n, shift = 0, scale=1, df=None, dfnull = 100, type='Normal', Norm = False):\n",
    "    var = VaR(alpha = 0.99, sigma2 = sigma2, mu = mu, df = dfnull, type = type, Norm = Norm)\n",
    "    var_back = []\n",
    "    if Norm == False:\n",
    "        for _ in range(n):\n",
    "            X_t = t.rvs(df, loc = shift, scale=scale, size=T)\n",
    "            exceedances = 0\n",
    "            for i in range (T):\n",
    "                if X_t[i] + var<0:\n",
    "                    exceedances += 1\n",
    "            var_back.append(exceedances)\n",
    "    if Norm == True:\n",
    "        for _ in range(n):\n",
    "            X_t = t.rvs(df, loc = shift, scale=np.sqrt((df-2)/df), size=T)\n",
    "            exceedances = 0\n",
    "            for i in range (T):\n",
    "                if X_t[i] + var<0:\n",
    "                    exceedances += 1\n",
    "            var_back.append(exceedances)\n",
    "    \n",
    "    var_back = np.array(var_back)\n",
    "    var_back = var_back[~np.isnan(var_back)]\n",
    "    return var_back, np.mean(var_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7,  9,  9, ...,  7, 10,  8]), 8.73124)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VaR_backtest(sigma2 = 1, mu = 0, T = 250, n = 100000, shift = 0, scale = 1, df = 3, dfnull = 10, type = 't', Norm = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z_1 - t-fordelt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_1(quan, sigma2, mu, T, n, shift = 0,  df=None, dfnull = 100, type='Normal'):\n",
    "    Z_1_list = []\n",
    "    var = VaR(alpha = 0.975, sigma2 = sigma2, mu=mu, df = dfnull, type = type)\n",
    "    es = ES(alpha = 0.975, sigma2= sigma2, mu= mu, df = dfnull, type = type)\n",
    "    \n",
    "    for _ in range(n):\n",
    "        X_t = t.rvs(df, loc = shift, size=T)\n",
    "        q=0\n",
    "        P = var + X_t\n",
    "        N_t = np.sum(P < 0)\n",
    "        \n",
    "\n",
    "        for i in range(T):\n",
    "            if X_t[i]+var<0:\n",
    "                q+= X_t[i]/es\n",
    "            Z_1 = q/N_t+1\n",
    "        Z_1_list.append(Z_1)\n",
    "    \n",
    "    Z_1_list = np.array(Z_1_list)\n",
    "    Z_1_list = Z_1_list[~np.isnan(Z_1_list)]\n",
    "    quant = np.quantile(Z_1_list,quan)\n",
    "    \n",
    "    return quant, Z_1_list, np.mean(Z_1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z_2 - t-fordelt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_2(quan, sigma2, mu, T, n, shift = 0, scale=1, df=None, dfnull = 100, Norm = False, type='Normal'):\n",
    "    Z_2_list = []\n",
    "    var = VaR(alpha = 0.975, sigma2 = sigma2, mu = mu, df = dfnull, type = type, Norm = Norm)\n",
    "    es =  ES( alpha = 0.975, sigma2 = sigma2, mu = mu, df = dfnull, type = type, Norm = Norm)\n",
    "    if Norm == False:\n",
    "        for _ in range(n):\n",
    "            X_t = t.rvs(df = df, size=T, scale = scale, loc = shift)\n",
    "            q=0\n",
    "            Talpha = T*0.025\n",
    "            \n",
    "            \n",
    "            for i in range(T):\n",
    "                if X_t[i]+var<0:\n",
    "                    q+= (X_t[i]/es)\n",
    "                Z_2 = q/(Talpha)+1\n",
    "            Z_2_list.append(Z_2)\n",
    "    \n",
    "    elif Norm == True:\n",
    "        for _ in range(n):\n",
    "            X_t = t.rvs(df = df, size=T, scale = np.sqrt((df-2)/df), loc = shift)\n",
    "            q=0\n",
    "            Talpha = T*0.025\n",
    "            \n",
    "            \n",
    "            for i in range(T):\n",
    "                if X_t[i]+var<0:\n",
    "                    q+= (X_t[i]/es)\n",
    "                Z_2 = q/(Talpha)+1\n",
    "            Z_2_list.append(Z_2)\n",
    "    Z_2_list = np.array(Z_2_list)\n",
    "    Z_2_list = Z_2_list[~np.isnan(Z_2_list)]\n",
    "    quant = np.quantile(Z_2_list,quan)\n",
    "    return quant , Z_2_list, np.mean(Z_2_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.11176119643873104"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_2(0.05, 1, 0, 250, 100000, shift = 0, scale=1, df=3, dfnull = 100, Norm = True, type='t')[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Z_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z_3(quan, T, n, df=None, dfnull=100, shift = 0, scale = 1, Norm = False):\n",
    "    Z_3_list = []\n",
    "    TAlpha = round(T*0.025)\n",
    "    if Norm == False:\n",
    "        integral = -T/TAlpha * integrate.quad(lambda p: sc.betainc(T-TAlpha, TAlpha, 1-p)*t.ppf(p, df=dfnull), 0, 1)[0]\n",
    "        for _ in range(n):\n",
    "            U = uniform.rvs(size=T)\n",
    "            q=0\n",
    "            PU = t.ppf(U, df=df, scale=scale, loc = shift)\n",
    "            PU = np.sort(PU)\n",
    "            for i in range(TAlpha):\n",
    "                q+= PU[i]\n",
    "            \n",
    "            ES_hat = -1/TAlpha*q\n",
    "            Z_3 = -ES_hat/integral+1\n",
    "            \n",
    "            Z_3_list.append(Z_3)\n",
    "    \n",
    "    if Norm == True:\n",
    "        integral = -T/TAlpha * integrate.quad(lambda p: sc.betainc(T-TAlpha, TAlpha, 1-p)*t.ppf(p, scale = np.sqrt((dfnull-2)/dfnull), df=dfnull), 0, 1)[0]\n",
    "        for _ in range(n):\n",
    "            U = uniform.rvs(size=T)\n",
    "            q=0\n",
    "            PU = t.ppf(U, df=df, scale=np.sqrt((df-2)/df), loc = shift)\n",
    "            PU = np.sort(PU)\n",
    "            for i in range(TAlpha):\n",
    "                q+= PU[i]\n",
    "            \n",
    "            ES_hat = -1/TAlpha*q\n",
    "            Z_3 = -ES_hat/integral+1\n",
    "            \n",
    "            Z_3_list.append(Z_3)\n",
    "    Z_3_list = np.array(Z_3_list)\n",
    "    Z_3_list = Z_3_list[~np.isnan(Z_3_list)]\n",
    "    quant = np.quantile(Z_3_list,quan)\n",
    "    return quant , Z_3_list, np.mean(Z_3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5163149607298756,\n",
       " array([-0.17683012,  0.04023883, -0.04485968, ..., -0.18420869,\n",
       "        -0.18164825,  0.1716104 ]),\n",
       " -0.15875521050120828)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_3(0.05, 250, 100000, df=5, dfnull=100, shift = 0, scale = 1, Norm = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power and CDF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Power:\n",
    "    def __init__(self, data_null, data_alternatives, significance_level, alternative_names=None):\n",
    "       \n",
    "        self.data_null = data_null\n",
    "        self.data_alternatives = data_alternatives\n",
    "        self.significance_level = significance_level\n",
    "        self.alternative_names = alternative_names if alternative_names else [f'Alternative {i+1}' for i in range(len(data_alternatives))]\n",
    "        self.sorted_null = np.sort(data_null)\n",
    "        self.crit_value = self.compute_critical_value()\n",
    "\n",
    "    def CDF(self, data):\n",
    "        data_sorted = np.sort(data)\n",
    "        return np.arange(1, len(data_sorted) + 1) / len(data_sorted)\n",
    "\n",
    "    def compute_critical_value(self):\n",
    "        cdf_0 = self.CDF(self.data_null)\n",
    "        crit_value_index = np.argmax(cdf_0 >= self.significance_level)\n",
    "        return self.sorted_null[crit_value_index]\n",
    "\n",
    "    def compute_powers(self):\n",
    "        powers = {'Significance Level': self.significance_level}\n",
    "        for name, data in zip(self.alternative_names, self.data_alternatives):\n",
    "            power = np.sum(data <= self.crit_value) / len(data)\n",
    "            powers[name] = power\n",
    "        return pd.DataFrame([powers])\n",
    "\n",
    "    def plot_cdfs(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        cdf_0 = self.CDF(self.data_null)\n",
    "        plt.plot(self.sorted_null, cdf_0, label='CDF of Null Data', color='blue')\n",
    "\n",
    "        for name, data in zip(self.alternative_names, self.data_alternatives):\n",
    "            sorted_data = np.sort(data)\n",
    "            cdf_data = self.CDF(data)\n",
    "            plt.plot(sorted_data, 1-cdf_data, label=f'1-CDF of {name}', color=np.random.rand(3,))\n",
    "        \n",
    "        plt.axvline(x=self.crit_value, color='black', linestyle='--', label=f'Critical Value = {self.crit_value:.2f}')\n",
    "        plt.title('Comparison of CDFs for Null and Alternative Hypotheses')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Cumulative Probability')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_null = VaR_backtest(sigma2 = 1, mu = 0, T = 250, n = 100000, shift = 0, scale = 1, df = 100, dfnull = 100, type = 't', Norm = False)[0]\n",
    "data_a_1 = VaR_backtest(sigma2 = 1, mu = 0, T = 250, n = 100000, shift = 0, scale = 1, df = 10, dfnull = 100, type = 't', Norm = False)[0]\n",
    "data_a_2 = VaR_backtest(sigma2 = 1, mu = 0, T = 250, n = 100000, shift = 0, scale = 1, df = 5, dfnull = 100, type = 't', Norm = False)[0]\n",
    "data_a_3 = VaR_backtest(sigma2 = 1, mu = 0, T = 250, n = 100000, shift = 0, scale = 1, df = 3, dfnull = 100, type = 't', Norm = False)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Significance Level   DF 10    DF 5    DF 3\n",
      "              0.041 0.37781 0.82016 0.98576\n",
      "              0.104 0.55242 0.90861 0.99485\n"
     ]
    }
   ],
   "source": [
    "names = ['DF 10', 'DF 5', 'DF 3']\n",
    "\n",
    "# Creating instances for different significance levels\n",
    "significance_levels = [0.041, 0.104]\n",
    "results = [Power(-data_null, [-data_a_1, -data_a_2, -data_a_3], sl, names).compute_powers() for sl in significance_levels]\n",
    "\n",
    "# Combining the results\n",
    "combined_results = pd.concat(results).reset_index(drop=True).to_string(index = False)\n",
    "print(combined_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
