{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Theory as th\n",
    "import numpy as np\n",
    "from scipy.stats import t, binom_test\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.special as sc\n",
    "import scipy.integrate as integrate\n",
    "from scipy.stats import uniform\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.power import TTestIndPower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_0 : Df = 100\n",
    "VaR_scale_100_25 = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = th.scale(0.025 , 100, 't'), df = 100, dfnull = 100, Norm = False, type = 't')\n",
    "VaR_scale_100_5 =  th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = th.scale(0.05 , 100, 't'),  df = 100, dfnull = 100, Norm = False, type = 't')\n",
    "VaR_scale_100_10 = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = th.scale(0.1 , 100, 't'),   df = 100, dfnull = 100, Norm = False, type = 't')\n",
    "\n",
    "# H_0: Df = 5\n",
    "VaR_scale_5_25 = th.VaR_backtest(sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = th.scale(0.025 , 5, 't'), df = 5, dfnull = 5, Norm = False, type = 't')\n",
    "VaR_scale_5_5 =  th.VaR_backtest(sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = th.scale(0.05 , 5, 't'),  df = 5, dfnull = 5, Norm = False, type = 't')\n",
    "VaR_scale_5_10 = th.VaR_backtest(sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = th.scale(0.1 , 5, 't'),   df = 5, dfnull = 5, Norm = False, type = 't')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student-t vs. Normalized Student-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_0: df = 100 - Standard student t:\n",
    "VaR_100_100 = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 100, dfnull = 100, Norm = False, type = 't')\n",
    "VaR_100_10 =  th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 10,  dfnull = 100, Norm = False, type = 't')\n",
    "VaR_100_5 =   th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 5,   dfnull = 100, Norm = False, type = 't')\n",
    "VaR_100_3 =   th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 3,   dfnull = 100, Norm = False, type = 't')\n",
    "\n",
    "# H_0: df = 100 - Standard student t:\n",
    "VaR_10_10 = th.VaR_backtest(sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 10, dfnull = 10, Norm = False, type = 't')\n",
    "VaR_10_5 =  th.VaR_backtest(sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 5,  dfnull = 10, Norm = False, type = 't')\n",
    "VaR_10_3 =  th.VaR_backtest(sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 3,  dfnull = 10, Norm = False, type = 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_0: df = 100 - Normalized student t:\n",
    "VaR_100_100_norm = th.VaR_backtest( sigma2 = 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 100, dfnull = 100, Norm = True, type = 't')\n",
    "VaR_100_10_norm =  th.VaR_backtest( sigma2 = 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 10,  dfnull = 100, Norm = True, type = 't')\n",
    "VaR_100_5_norm =   th.VaR_backtest( sigma2 = 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 5,   dfnull = 100, Norm = True, type = 't')\n",
    "VaR_100_3_norm =   th.VaR_backtest( sigma2 = 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 3,   dfnull = 100, Norm = True, type = 't')\n",
    "\n",
    "# H_0: df = 10 - Normalized student t:\n",
    "VaR_10_10_norm = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 10, dfnull = 10, Norm = True, type = 't')\n",
    "VaR_10_5_norm =  th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 5, dfnull = 10, Norm = True, type = 't')\n",
    "VaR_10_3_norm =  th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = 0 , scale  = 1, df = 3, dfnull = 10, Norm = True, type = 't')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed VaR 2.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# H_0: df = 100 - fixed VaR 2.5%:\n",
    "VaR_100_100_const_VaR = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = th.shift(0.975, df=100, dfnull = 100) , scale  = 1, df = 100, dfnull = 100, Norm = False, type = 't')\n",
    "VaR_100_10_const_VaR = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = th.shift(0.975, df=10, dfnull = 100) , scale  = 1, df = 10, dfnull = 100, Norm = False, type = 't')\n",
    "VaR_100_5_const_VaR = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = th.shift(0.975, df=5, dfnull = 100) , scale  = 1, df = 5, dfnull = 100, Norm = False, type = 't')\n",
    "VaR_100_3_const_VaR = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = th.shift(0.975, df=3, dfnull = 100) , scale  = 1, df = 3, dfnull = 100, Norm = False, type = 't')\n",
    "\n",
    "# H_0: df = 10 - fixed VaR 2.5%:\n",
    "VaR_10_10_const_VaR = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = th.shift(0.975, df=10, dfnull = 10) , scale  = 1, df = 10, dfnull = 10, Norm = False, type = 't')\n",
    "VaR_10_5_const_VaR = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = th.shift(0.975, df=5, dfnull = 10) , scale  = 1, df = 5, dfnull = 10, Norm = False, type = 't')\n",
    "VaR_10_3_const_VaR = th.VaR_backtest( sigma2= 1 , mu = 0, T = 250, n = 100000, shift = th.shift(0.975, df=3, dfnull = 10) , scale  = 1, df = 3, dfnull = 10, Norm = False, type = 't')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " df  Significance Level Source  Power 5%  Power 10%\n",
      "  5                 4.1    VaR   0.37841    0.93121\n",
      "                   10.6    VaR   0.55591    0.97014\n",
      "100                 3.9    VaR   0.38626    0.93996\n",
      "                   10.5    VaR   0.56441    0.97466\n"
     ]
    }
   ],
   "source": [
    "#df = 5:\n",
    "names = ['Power 5%', 'Power 10%']\n",
    "significance_levels = [0.041, 0.106]\n",
    "\n",
    "table1_5 = [th.Power(-VaR_scale_5_25[0], [-VaR_scale_5_5[0], -VaR_scale_5_10[0]], sl, names).compute_powers() for sl in significance_levels]\n",
    "\n",
    "#Combine Z_1 and Z_2\n",
    "table1_5 = pd.concat(table1_5).reset_index(drop=True)\n",
    "\n",
    "table1_5['Source'] = 'VaR'\n",
    "\n",
    "table1_5 = table1_5.sort_values(by=['Significance Level', 'Source'])\n",
    "\n",
    "table1_5 = table1_5[['Significance Level', 'Source', 'Power 5%', 'Power 10%']]\n",
    "table1_5['Significance Level'] *= 100\n",
    "\n",
    "#df = 100:\n",
    "names = ['Power 5%', 'Power 10%']\n",
    "significance_levels = [0.039, 0.105]\n",
    "\n",
    "table1_100 = [th.Power(-VaR_scale_100_25[0], [-VaR_scale_100_5[0], -VaR_scale_100_10[0]], sl, names).compute_powers() for sl in significance_levels]\n",
    "\n",
    "#Combine Z_2 and Z_3\n",
    "table1_100 = pd.concat(table1_100).reset_index(drop=True)\n",
    "\n",
    "table1_100['Source'] = 'VaR'\n",
    "\n",
    "table1_100 = table1_100.sort_values(by=['Significance Level', 'Source'])\n",
    "\n",
    "table1_100 = table1_100[['Significance Level', 'Source', 'Power 5%', 'Power 10%']]\n",
    "table1_100['Significance Level'] *= 100\n",
    "\n",
    "#Final Table\n",
    "table_1 = pd.concat([table1_5, table1_100], ignore_index=True)\n",
    "\n",
    "table_1.insert(0, 'df', [5]+ [''] + [100] + [''])\n",
    "table_1 = table_1.to_string(index=False)\n",
    "print(table_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student-t norm and non norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution  df  Significance Level Source  Power df = 5  Power df = 3  Power df = 10\n",
      "   Student-t  10                 4.0    VaR       0.37676       0.87105            NaN\n",
      "                                10.6    VaR       0.55479       0.93820            NaN\n",
      "             100                 4.1    VaR           NaN       0.98606        0.37948\n",
      "                                10.4    VaR           NaN       0.99514        0.55492\n",
      "Distribution  df  Significance Level Source  Power df = 5  Power df = 3  Power df = 10\n",
      "Normlaized t  10                 4.0    VaR       0.08651       0.07573            NaN\n",
      "                                10.6    VaR       0.18823       0.17009            NaN\n",
      "             100                 4.1    VaR           NaN       0.12626        0.10756\n",
      "                                10.7    VaR           NaN       0.25146        0.22231\n"
     ]
    }
   ],
   "source": [
    "#df = 10:\n",
    "names = ['Power df = 5', 'Power df = 3']\n",
    "significance_levels = [0.040, 0.106]\n",
    "\n",
    "table2_10 = [th.Power(-VaR_10_10[0], [-VaR_10_5[0], -VaR_10_3[0]], sl, names).compute_powers() for sl in significance_levels]\n",
    "\n",
    "#Combine Z_1 and Z_2\n",
    "table2_10 = pd.concat(table2_10).reset_index(drop=True)\n",
    "\n",
    "table2_10['Source'] = 'VaR'\n",
    "\n",
    "table2_10 = table2_10.sort_values(by=['Significance Level', 'Source'])\n",
    "\n",
    "table2_10 = table2_10[['Significance Level', 'Source', 'Power df = 5', 'Power df = 3']]\n",
    "table2_10['Significance Level'] *= 100\n",
    "\n",
    "#df = 100:\n",
    "names = ['Power df = 10', 'Power df = 3']\n",
    "significance_levels = [0.041, 0.104]\n",
    "\n",
    "table2_100 = [th.Power(-VaR_100_100[0], [-VaR_100_10[0], -VaR_100_3[0]], sl, names).compute_powers() for sl in significance_levels]\n",
    "\n",
    "#Combine Z_2 and Z_3\n",
    "table2_100 = pd.concat(table2_100).reset_index(drop=True)\n",
    "\n",
    "table2_100['Source'] = 'VaR'\n",
    "\n",
    "table2_100 = table2_100.sort_values(by=['Significance Level', 'Source'])\n",
    "\n",
    "table2_100 = table2_100[['Significance Level', 'Source', 'Power df = 10', 'Power df = 3']]\n",
    "table2_100['Significance Level'] *= 100\n",
    "\n",
    "table_2 = pd.concat([table2_10, table2_100], ignore_index=True)\n",
    "\n",
    "table_2.insert(0, 'df', [10]+ [''] + [100] + [''])\n",
    "table_2.insert(0,'Distribution', ['Student-t']+['']*3)\n",
    "table_2 = table_2.to_string(index=False)\n",
    "print(table_2)\n",
    "\n",
    "#df = 10 normalized:\n",
    "names = ['Power df = 5', 'Power df = 3']\n",
    "significance_levels = [0.04, 0.106]\n",
    "\n",
    "table2_10_norm = [th.Power(-VaR_10_10_norm[0], [-VaR_10_5_norm[0], -VaR_10_3_norm[0]], sl, names).compute_powers() for sl in significance_levels]\n",
    "\n",
    "#Combine Z_1 and Z_2\n",
    "table2_10_norm = pd.concat(table2_10_norm).reset_index(drop=True)\n",
    "\n",
    "table2_10_norm['Source'] = 'VaR'\n",
    "\n",
    "table2_10_norm = table2_10_norm.sort_values(by=['Significance Level', 'Source'])\n",
    "\n",
    "table2_10_norm = table2_10_norm[['Significance Level', 'Source', 'Power df = 5', 'Power df = 3']]\n",
    "table2_10_norm['Significance Level'] *= 100\n",
    "\n",
    "#df = 100:\n",
    "names = ['Power df = 10', 'Power df = 3']\n",
    "significance_levels = [0.041, 0.107]\n",
    "\n",
    "table2_100_norm = [th.Power(-VaR_100_100_norm[0], [-VaR_100_10_norm[0], -VaR_100_3_norm[0]], sl, names).compute_powers() for sl in significance_levels]\n",
    "\n",
    "#Combine Z_2 and Z_3\n",
    "table2_100_norm = pd.concat(table2_100_norm).reset_index(drop=True)\n",
    "\n",
    "table2_100_norm['Source'] = 'VaR'\n",
    "\n",
    "table2_100_norm= table2_100_norm.sort_values(by=['Significance Level', 'Source'])\n",
    "\n",
    "table2_100_norm = table2_100_norm[['Significance Level', 'Source', 'Power df = 10', 'Power df = 3']]\n",
    "table2_100_norm['Significance Level'] *= 100\n",
    "\n",
    "table_2_norm = pd.concat([table2_10_norm, table2_100_norm], ignore_index=True)\n",
    "\n",
    "table_2_norm.insert(0, 'df', [10]+ [''] + [100] + [''])\n",
    "table_2_norm.insert(0,'Distribution', ['Normlaized t']+['']*3)\n",
    "table_2_norm = table_2_norm.to_string(index=False)\n",
    "print(table_2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed VaR 2.5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution  df  Significance Level Source  Power df = 5  Power df = 3\n",
      "   Student-t  10                 4.0     Z1       0.12065       0.25108\n",
      "                                10.6     Z1       0.24535       0.41788\n",
      "             100                 4.1    VaR       0.11036       0.33356\n",
      "                                10.8    VaR       0.22968       0.50965\n"
     ]
    }
   ],
   "source": [
    "#df = 10:\n",
    "names = ['Power df = 5', 'Power df = 3']\n",
    "significance_levels = [0.04, 0.106]\n",
    "\n",
    "table3_10_Z1 = [th.Power(-VaR_10_10_const_VaR[0], [-VaR_10_5_const_VaR[0], -VaR_10_3_const_VaR[0]], sl, names).compute_powers() for sl in significance_levels]\n",
    "\n",
    "#Combine Z_1 and Z_2\n",
    "table3_10 = pd.concat(table3_10_Z1).reset_index(drop=True)\n",
    "\n",
    "table3_10['Source'] = 'Z1'\n",
    "\n",
    "\n",
    "table3_10 = table3_10.sort_values(by=['Significance Level', 'Source'])\n",
    "\n",
    "table3_10 = table3_10[['Significance Level', 'Source', 'Power df = 5', 'Power df = 3']]\n",
    "table3_10['Significance Level'] *= 100\n",
    "\n",
    "#df = 100:\n",
    "names = ['Power df = 5', 'Power df = 3']\n",
    "significance_levels = [0.041, 0.108]\n",
    "\n",
    "table3_100 = [th.Power(-VaR_100_100_const_VaR[0], [-VaR_100_10_const_VaR[0], -VaR_100_3_const_VaR[0]], sl, names).compute_powers() for sl in significance_levels]\n",
    "\n",
    "#Combine Z_1 and Z_2\n",
    "table3_100 = pd.concat(table3_100).reset_index(drop=True)\n",
    "\n",
    "table3_100['Source'] = 'VaR'\n",
    "\n",
    "table3_100 = table3_100.sort_values(by=['Significance Level', 'Source'])\n",
    "\n",
    "table3_100 = table3_100[['Significance Level', 'Source', 'Power df = 5', 'Power df = 3']]\n",
    "table3_100['Significance Level'] *= 100\n",
    "\n",
    "table_3 = pd.concat([table3_10, table3_100], ignore_index=True)\n",
    "\n",
    "table_3.insert(0, 'df', [10]+ [''] + [100] + [''])\n",
    "table_3.insert(0,'Distribution', ['Student-t']+['']*3)\n",
    "table_3 = table_3.to_string(index=False)\n",
    "print(table_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
